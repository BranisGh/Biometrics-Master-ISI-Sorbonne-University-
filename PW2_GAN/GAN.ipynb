{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSJ9sFDlfUn-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QOuNlHwfUoA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BIoMétrie : Generative Adversarial Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L’objectif de ce TP est d’implémenter une architecture neuronale complexe, comme les réseaux génératifs antagonistes, pour apprendre à générer des images. Le TP est aussi l’occasion d’appréhender en TF l’apprentissage de réseaux complexes avec des fonctions de pertes qui n’affectent qu’une partie des poids du réseau seulement. Nous utiliserons la base de chiffres MNIST\n",
        "accessibles depuis le module dataset de Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBBGrGRIfUoA"
      },
      "source": [
        "### Installer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "gFAJwCJMfUoC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Jk6gl41OfUoD",
        "outputId": "b4bf2bd0-6198-4b83-d94f-a629d413770d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.10.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tQw6HE3KfUoE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Brani\\AppData\\Local\\Temp\\ipykernel_83628\\3514171520.py:13: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn')\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow.keras.datasets as tfds\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RowGbK7yfUoF"
      },
      "source": [
        "### Charger et préparer le jeu de données\n",
        "Nous utiliserons le jeu de données `MNIST` pour former le générateur et le discriminateur. Le générateur générera des chiffres manuscrits ressemblant aux données MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sFv5P_YqfUoF"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"313.38pt\" height=\"313.38pt\" viewBox=\"0 0 313.38 313.38\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-12-06T20:04:49.342346</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.6.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 313.38 \nL 313.38 313.38 \nL 313.38 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g clip-path=\"url(#p7ce76c7a81)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image2af1a4ddd1\" transform=\"scale(1 -1) translate(0 -299)\" x=\"7.2\" y=\"-7.18\" width=\"299\" height=\"299\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p7ce76c7a81\">\n   <rect x=\"7.2\" y=\"7.2\" width=\"298.98\" height=\"298.98\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(train_images[0], cmap='gray')\n",
        "plt.axis(False)\n",
        "plt.grid(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAkzFVdMfUoG",
        "outputId": "86de2b72-ae24-4530-ea6c-ac6353733e3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images.shape, train_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_images, rows, cols = train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zfQKxlDkfUoG"
      },
      "outputs": [],
      "source": [
        "# Transformation sous forme tensor (nb_images, h,w, ch)\n",
        "train_images = train_images.reshape(nb_images, rows, cols, 1).astype('float32')\n",
        "# Normalisation des images\n",
        "train_images = train_images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MNtzFEmnfUoH"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = nb_images # 60000\n",
        "BATCH_SIZE = 256\n",
        "INPUT_SHAPE_GENERATOR = 32\n",
        "INPUT_SHAPE_DISCRIMINATOR = (rows, cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7wMyE_ZyfUoH"
      },
      "outputs": [],
      "source": [
        "# Batch ans shuffle the data\n",
        "train_images = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "train_images = list(train_images.as_numpy_iterator())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fdtnKKVfUoH",
        "outputId": "601382ec-4e1a-4221-b41d-e88d11ca0088"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"313.38pt\" height=\"313.38pt\" viewBox=\"0 0 313.38 313.38\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-12-06T20:04:58.823837</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.6.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 313.38 \nL 313.38 313.38 \nL 313.38 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g clip-path=\"url(#p5a5f24abab)\">\n    <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAASsAAAErCAYAAACGpKW0AAAH2UlEQVR4nO3dzYvNbxzH4TMYD81QJAtStsos7EgxC7KRiEZs0MhiyoKVtSILxMpCsxArpJCFrBQLpShPyU7KQ41iZhRj+P0Hfp8zzel4z7mu9bv73DXTq+/ifDtdjUbjTwPgHzer3RcAqBArIIJYARHECoggVkAEsQIiiBUQQayACGIFRBArIIJYARHECoggVkAEsQIiiBUQQayACGIFRBArIIJYARHECoggVkAEsQIiiBUQQayACGIFRBArIIJYARHECoggVkAEsQIiiBUQYU67L5Ckq6urtOvu7m7xTTrT79+/S7tfv361+Ca0gycrIIJYARHECoggVkAEsQIiiBUQQayACGIFRBArIIJYARE6/nWbefPmlbf79u0r7YaHh6d6Hf7i7du3pd3JkyfLZ968ebO8nZiYKG9//PhR3lLjyQqIIFZABLECIogVEEGsgAhiBUQQKyCCWAERxAqI0NVoNP60+xJV1R9s2L17d/nM48ePl7dr164t7cbHx8tn/vz5s7ztdIsWLSrtZs+e3ZLPv3LlSnk7NDRU2jXzv9LpPFkBEcQKiCBWQASxAiKIFRBBrIAIYgVEECsgglgBEcQKiBD1us2SJUtKu5GRkfKZ3759K29v3LhR2p09e7Z85qtXr8rbTjcwMFDabdq0qXzm4cOHy9tmXuN58eJFabd+/frymc28mtPb21vajY2Nlc9sN09WQASxAiKIFRBBrIAIYgVEECsgglgBEcQKiCBWQASxAiLMafcFmnH06NHS7tOnT+UzN2/eXN5WX6GgNa5duzatu0aj0fjy5Ut528wvIa1Zs6a0O3DgQPnMixcvlreDg4Ol3YULF8pntpsnKyCCWAERxAqIIFZABLECIogVEEGsgAhiBUQQKyBC1A9GVL+ZfOLEifKZvpVOVTPf9j5y5Mi0f/6qVavK23fv3k3757ebJysgglgBEcQKiCBWQASxAiKIFRBBrIAIYgVEECsgglgBEaJ+MGJgYKDdV6CDPX/+vK2fPzQ0VN428+MWKTxZARHECoggVkAEsQIiiBUQQayACGIFRBArIIJYARHECogQ9boNtNPTp0/L2/Hx8dKup6dnqtfpOJ6sgAhiBUQQKyCCWAERxAqIIFZABLECIogVEEGsgAi+wQ5F27ZtK299M336ebICIogVEEGsgAhiBUQQKyCCWAERxAqIIFZABLECIogVEMHrNky7xYsXl7d9fX0tvMn/GxoaKm+3b9/ewpv8v6tXr7b189vNkxUQQayACGIFRBArIIJYARHECoggVkAEsQIiiBUQQayACF636XCnT58ub9etW1faJb1u027NvMLz+vXrFt7k3+fJCoggVkAEsQIiiBUQQayACGIFRBArIIJYARHECoggVkAEr9t0uAULFpS3GzdubOFN2uPBgwfl7cOHD8vbu3fvlnZPnjwpnzk5OVnezkSerIAIYgVEECsgglgBEcQKiCBWQASxAiKIFRBBrIAIXY1G40+7L0H79Pb2lrdbtmwp7Y4dO1Y+c8OGDeVt1cuXL8vb/v7+8nZkZGQKt2G6eLICIogVEEGsgAhiBUQQKyCCWAERxAqIIFZABLECIogVEGFGvm7T3d1d3p47d668vX//fml3+/bt8pkzUTOv8Fy6dKm83bNnz1Su81c7d+4sb2/dujXtn0+dJysgglgBEcQKiCBWQASxAiKIFRBBrIAIYgVEECsgglgBEWbk6zYLFy4sb79+/VrefvjwobRbsWJF+cxO18yrOY8fPy7tVq9eXT5zdHS0vB0YGChv7927V95S48kKiCBWQASxAiKIFRBBrIAIYgVEECsgglgBEcQKiDCn3RdIsmzZstLu2bNn5TNPnjxZ3l6/fr28TTE2NlbenjlzprQbHh4un9nM2w67du0qb32Dffp5sgIiiBUQQayACGIFRBArIIJYARHECoggVkAEsQIiiBUQYUb+YMT8+fPL2+qPEDQajUZfX99UrvNXk5OT5e3nz59Lu2Zey3nz5k15e+fOndLu/fv35TObUf27NvO6zd69e8vbR48elbdbt24t7b5//14+s9N5sgIiiBUQQayACGIFRBArIIJYARHECoggVkAEsQIiiBUQYUa+btOMpUuXlrenTp0q7Q4dOjTV6/zTRkdHS7uJiYkW3+Tv5s6dW9729va25A7Lly8v7T5+/NiSz5+JPFkBEcQKiCBWQASxAiKIFRBBrIAIYgVEECsgglgBETr+G+zNmDWr1vaenp7ymTt27Chv+/v7S7uDBw+Wz6Tu8uXL5e3g4GBp9/v376lep+N4sgIiiBUQQayACGIFRBArIIJYARHECoggVkAEsQIiiBUQwes2Qaqv+3R3d7fk81euXFna7d+/vyWf327nz58vb0dGRlp3kQ7lyQqIIFZABLECIogVEEGsgAhiBUQQKyCCWAERxAqIIFZABK/bABE8WQERxAqIIFZABLECIogVEEGsgAhiBUQQKyCCWAERxAqIIFZABLECIogVEEGsgAhiBUQQKyCCWAERxAqIIFZABLECIogVEEGsgAhiBUQQKyCCWAERxAqIIFZABLECIogVEEGsgAhiBUQQKyCCWAERxAqIIFZABLECIogVEEGsgAhiBUQQKyCCWAERxAqIIFZABLECIogVEEGsgAhiBUQQKyCCWAERxAqIIFZABLECIvwHWwH6qL9t7RcAAAAASUVORK5CYII=\" id=\"image9f81414c41\" transform=\"scale(1 -1) translate(0 -299)\" x=\"7.2\" y=\"-7.18\" width=\"299\" height=\"299\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5a5f24abab\">\n   <rect x=\"7.2\" y=\"7.2\" width=\"298.98\" height=\"298.98\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(train_images[0][0], cmap='gray')\n",
        "plt.axis(False)\n",
        "plt.grid(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qafvzBcxfUoI"
      },
      "source": [
        "### Instanciation de G et D\n",
        "Le générateur et le discriminateur sont définis à l'aide de l' API séquentielle Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Le générateur\n",
        "Le générateur utilise `tf.keras.layers.Conv2DTranspose` (suréchantillonnage) pour produire une image à partir d'une graine (bruit aléatoire). Commencez avec une couche Dense qui prend cette graine en entrée, puis suréchantillonnez plusieurs fois jusqu'à ce que vous atteigniez la taille d'image souhaitée de 28x28x1. Notez l'activation de `tf.keras.layers.LeakyReLU` pour chaque couche, à l'exception de la couche de sortie qui utilise `tanh`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 1. Créer un générateur G avec une architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2d9TPKUpfUoI"
      },
      "outputs": [],
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    model.add(layers.Dense(14*14*128, use_bias=False, input_shape=(INPUT_SHAPE_GENERATOR,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((14, 14, 128)))\n",
        "    assert model.output_shape == (None, 14, 14, 128)  # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(256, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 256)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='sigmoid'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzieA9cvfUoJ"
      },
      "source": [
        "Utilisez le générateur (pas encore formé) pour créer une image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "NF22VugRfUoJ",
        "outputId": "6911b961-3da6-467d-9d06-72ab00a5791f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 25088)             802816    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 25088)            100352    \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 25088)             0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 14, 14, 256)      819200    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 14, 14, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 14, 14, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 14, 14, 128)      819200    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 14, 14, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 14, 14, 64)       204800    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 14, 14, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 28, 28, 1)        1600      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 28, 28, 1)        4         \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,749,764\n",
            "Trainable params: 2,698,690\n",
            "Non-trainable params: 51,074\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "generator = make_generator_model()\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "plot_model(generator, show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwyehYCKfUoK",
        "outputId": "ba0baf20-61b0-4616-b13e-b11b661bb49f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"313.38pt\" height=\"313.38pt\" viewBox=\"0 0 313.38 313.38\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-12-06T20:05:08.814871</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.6.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 313.38 \nL 313.38 313.38 \nL 313.38 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g clip-path=\"url(#p6e0986f54f)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image84a3a545c6\" transform=\"scale(1 -1) translate(0 -299)\" x=\"7.2\" y=\"-7.18\" width=\"299\" height=\"299\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6e0986f54f\">\n   <rect x=\"7.2\" y=\"7.2\" width=\"298.98\" height=\"298.98\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "noise = tf.random.normal([1, INPUT_SHAPE_GENERATOR])\n",
        "generated_image = generator(noise, training=False)\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
        "plt.grid(False); plt.axis(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "983uy5cTfUoK"
      },
      "source": [
        "#### Le discriminateur\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2. Créer un discriminateur D avec une architecture CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lk0OqhFUfUoL"
      },
      "outputs": [],
      "source": [
        "def make_discriminator_model():\n",
        "    model=tf.keras.Sequential()\n",
        "\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(1, 1), padding='same', input_shape=[28, 28, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.BatchNormalization())\n",
        "    \n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(1, 1), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "    model.add(layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1024, activation=tf.nn.relu))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz6uyVLhfUoL"
      },
      "source": [
        "Utilisez le discriminateur (pas encore formé) pour classer les images générées comme réelles ou fausses. Le modèle sera formé pour générer des valeurs positives pour les images réelles et des valeurs négatives pour les images factices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohOv4rp3fUoL",
        "outputId": "328aa44a-9ec5-4cd7-ca88-284409f35935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 64)        1664      \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 28, 28, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 28, 28, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 128)       204928    \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 28, 28, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 28, 28, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 28, 28, 256)       295168    \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 28, 28, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 14, 14, 512)       1180160   \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 14, 14, 512)      2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 7, 7, 512)        2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1024)              25691136  \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 1025      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29,739,777\n",
            "Trainable params: 29,736,833\n",
            "Non-trainable params: 2,944\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = make_discriminator_model()\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "plot_model(discriminator, show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.49705392]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "decision = discriminator(generated_image)\n",
        "print(decision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Création du GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Maintenant que nous avons instancié les deux modules élémentaires `generator` et `decriminator`, nous allons pouvoir créer le réseau GAN utilisé pour l’apprentissage antagoniste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeO-atp5fUoL"
      },
      "source": [
        "#### Define the loss and optimizers\n",
        "Define loss functions and optimizers for both models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 3. Pour la partie du discriminateur D, simplement compiler le module D avec la loss binary_crossentropy et l’optimiseur Adam. Utiliser la fonction summary pour visualiser l’architecture de votre réseau D.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qkKXodTYfUoM"
      },
      "outputs": [],
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False) # car sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "discriminator.compile(optimizer='Adam', loss=cross_entropy, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 64)        1664      \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 28, 28, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 28, 28, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 128)       204928    \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 28, 28, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 28, 28, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 28, 28, 256)       295168    \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 28, 28, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 14, 14, 512)       1180160   \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 14, 14, 512)      2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 7, 7, 512)        2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1024)              25691136  \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 1025      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29,739,777\n",
            "Trainable params: 29,736,833\n",
            "Non-trainable params: 2,944\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 4. Pour la partie du générateur G, l’apprentissage consiste à optimiser les poids du générateur G pour un état donné du discriminateur D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "— Faire une recopie du discriminateur D pour créer le discriminateur\n",
        "discriminateur_gan.  \n",
        "— Déclarer les poids de ce discriminateur comme non-entrainables, en fixant la valeur de son attribut trainable à False.  \n",
        "— Instancier le réseau GAN, avec comme entrée la taille du code latent aléatoire et comme sortie la composition de discriminator_gan(generator(gan_input)).  \n",
        "— Définir la loss binary_crossentropy et l’optimiseur Adam.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import clone_model\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, concatenate\n",
        "import numpy as np\n",
        "\n",
        "discriminator_gan = clone_model(discriminator)\n",
        "discriminator_gan.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32)]              0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 28, 28, 1)         2749764   \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 1)                 29739777  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 32,489,541\n",
            "Trainable params: 2,698,690\n",
            "Non-trainable params: 29,790,851\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "gan_input = tf.keras.layers.Input([INPUT_SHAPE_GENERATOR,])\n",
        "GAN = discriminator_gan(generator(gan_input))\n",
        "GAN = Model(gan_input, GAN)\n",
        "GAN.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "tf.keras.utils.plot_model(GAN, to_file= 'GAN.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "GAN.compile(optimizer='Adam', loss=cross_entropy, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "plot_model(GAN, show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Apprentissage du GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous allons maintenant procédé à l’apprentissage alterné des poids du générateur G et du discriminateur. Pour ce faire, nous allons créer notre propre boucle pour traiter les batches. Nous reproduirons pour chaque batches les étapes suivantes :  \n",
        "\n",
        "5. Réaliser un tirage aléatoire du code latent, en utilisant un tirage aléatoire dans une distribution normale. Pour ce faire on utilisera la fonction `random de numpy` ou `random de tensorflow`.  \n",
        "6. Réaliser la prédiction des images générées à partir de `G` et du code latent.  \n",
        "7. Construire un mélange d’images réelles tirées aléatoirement et d’images générées avec les labels correspondant.  \n",
        "8. Entrainer le discriminateur `D` à partir de ce mélange. On utilisera la méthode `train_on_batch`.  \n",
        "9. Réaliser un nouveau tirage aléatoire du code latent, et créer les labels correspondants à \"réel\".  \n",
        "10. Entrainer le réseau antagoniste à partir de ces données. On utilisera la méthode `train_on_batch`.  \n",
        "Nous ferons un affichage des losses du discriminateur D et réseau antagoniste GAN toutes les 10 époques, et un tracé de 10 images produites par le générateur.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "# @tf.function\n",
        "def train_step(real_images, gan, generator):\n",
        "  noise = np.random.normal(loc=0.0, scale=1.0, size=(BATCH_SIZE, INPUT_SHAPE_GENERATOR))\n",
        "  generated_images = generator(noise, training=True)\n",
        "  combined_images = np.concatenate([generated_images.numpy(), real_images])\n",
        "  labels = np.concatenate([np.zeros((BATCH_SIZE, 1)),\n",
        "                           np.ones((BATCH_SIZE, 1))])\n",
        "  labels += 0.05 * np.random.random(labels.shape) # astuce trover sur internet \n",
        "  d_loss, d_acc = discriminator.train_on_batch(combined_images, labels)\n",
        "  noise = np.random.normal(loc=0.0, scale=1.0, size=(BATCH_SIZE, INPUT_SHAPE_GENERATOR))\n",
        "  misleading_labels = np.ones((BATCH_SIZE, 1))\n",
        "  gan_loss, gan_acc = gan.train_on_batch(noise, misleading_labels)\n",
        "\n",
        "  return d_loss, d_acc, gan_loss, gan_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Générer et enregistrer des images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 255.0, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Générer et enregistrer des images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def train(dataset, gan, generator, epochs=50):\n",
        "  history = { 'd_loss'   : [],\n",
        "              'd_acc'   : [],\n",
        "              'gan_loss' : [],\n",
        "              'gan_acc'  : [] }\n",
        "  \n",
        "  checkpoint_dir = './training_checkpoints'\n",
        "  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "  checkpoint = tf.train.Checkpoint(generator_optimizer=tf.keras.optimizers.Adam(),\n",
        "                                   gan_optimizer=tf.keras.optimizers.Adam(), \n",
        "                                   generator=generator,\n",
        "                                   gan=gan) \n",
        "  \n",
        "  seed = tf.random.normal([BATCH_SIZE, INPUT_SHAPE_GENERATOR])\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    print(f'epoch {epoch+1}')\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      d_loss, d_acc, gan_loss, gan_acc  = train_step(image_batch, gan, generator)\n",
        "      history['d_loss'].append(d_loss), history['d_acc'].append(d_acc)\n",
        "      history['gan_loss'].append(gan_loss), history['gan_acc'].append(gan_acc)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)\n",
        "\n",
        "  # generator.save('models/generator.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "  return generator, gan, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjNrkQ3xfUoR",
        "outputId": "dcf94539-c943-449e-b3e2-ab3e6d3decfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "# tf.test.is_gpu_available()\n",
        "tf.config.list_physical_devices('GPU')\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bra\n",
            "bra\n",
            "bra\n",
            "branis\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "bra\n",
            "bra\n",
            "bra\n",
            "bra\n",
            "bra\n",
            "bra\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'d_acc'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Brani\\Downloads\\Biometrics-Master-ISI-Sorbonne-University-\\PW2_GAN\\GAN.ipynb Cellule 55\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Brani/Downloads/Biometrics-Master-ISI-Sorbonne-University-/PW2_GAN/GAN.ipynb#Y152sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m generator, gan, history \u001b[39m=\u001b[39m train(train_images, GAN, generator, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
            "\u001b[1;32mc:\\Users\\Brani\\Downloads\\Biometrics-Master-ISI-Sorbonne-University-\\PW2_GAN\\GAN.ipynb Cellule 55\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, gan, generator, epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Brani/Downloads/Biometrics-Master-ISI-Sorbonne-University-/PW2_GAN/GAN.ipynb#Y152sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m image_batch \u001b[39min\u001b[39;00m dataset:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Brani/Downloads/Biometrics-Master-ISI-Sorbonne-University-/PW2_GAN/GAN.ipynb#Y152sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m   d_loss, d_acc, gan_loss, gan_acc  \u001b[39m=\u001b[39m train_step(image_batch, gan, generator)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Brani/Downloads/Biometrics-Master-ISI-Sorbonne-University-/PW2_GAN/GAN.ipynb#Y152sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   history[\u001b[39m'\u001b[39m\u001b[39md_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(d_loss), history[\u001b[39m'\u001b[39;49m\u001b[39md_acc\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mappend(d_acc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Brani/Downloads/Biometrics-Master-ISI-Sorbonne-University-/PW2_GAN/GAN.ipynb#Y152sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   history[\u001b[39m'\u001b[39m\u001b[39mgan_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(gan_loss), history[\u001b[39m'\u001b[39m\u001b[39mgan_acc\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(gan_acc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Brani/Downloads/Biometrics-Master-ISI-Sorbonne-University-/PW2_GAN/GAN.ipynb#Y152sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Produce images for the GIF as you go\u001b[39;00m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'd_acc'"
          ]
        }
      ],
      "source": [
        "generator, gan, history = train(train_images, GAN, generator, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Brani\\Downloads\\Biometrics-Master-ISI-Sorbonne-University-\\PW2_GAN\\GAN.ipynb Cellule 56\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Brani/Downloads/Biometrics-Master-ISI-Sorbonne-University-/PW2_GAN/GAN.ipynb#Y153sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history\u001b[39m.\u001b[39mplot()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ],
      "source": [
        "history.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 ('env_ML')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "3aeec228df1f54caf4e0351da5a2d0a118f716cc91cd17ac0ce0027b95db8302"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
